{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Gio\n",
      "[nltk_data]     Gerardino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Gio\n",
      "[nltk_data]     Gerardino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Gio\n",
      "[nltk_data]     Gerardino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Gio\n",
      "[nltk_data]     Gerardino\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>üòç</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Praise towards the creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yummy balut watching from Davao, philippines</td>\n",
       "      <td>Yummy balut watching from Davao philippines</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mostly, when a product is introduced or seen o...</td>\n",
       "      <td>Mostly when a product is introduced or seen on...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi√¨iii can you do a challenge that you&amp;#39;ll ...</td>\n",
       "      <td>Hiiii can you do a challenge that you'll only ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Request/Suggestion towards the creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Who here came back when they get bored?</td>\n",
       "      <td>Who here came back when they get bored</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Kz sang iconic music in Philippines.. Emotiona...</td>\n",
       "      <td>Kz sang iconic music in Philippines Emotional ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>The piggy though ;(</td>\n",
       "      <td>The piggy though</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>HARAM</td>\n",
       "      <td>HARAM</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  \\\n",
       "0                                                     üòç   \n",
       "1     the jollibee taste testing is the reason why i...   \n",
       "2          Yummy balut watching from Davao, philippines   \n",
       "3     Mostly, when a product is introduced or seen o...   \n",
       "4     Hi√¨iii can you do a challenge that you&#39;ll ...   \n",
       "...                                                 ...   \n",
       "9995            Who here came back when they get bored?   \n",
       "9996  Kz sang iconic music in Philippines.. Emotiona...   \n",
       "9997                                The piggy though ;(   \n",
       "9998  In the Philippines you could buy a whole Lecho...   \n",
       "9999                                              HARAM   \n",
       "\n",
       "                                          clean_comment  Polarity  \\\n",
       "0                                                   NaN       NaN   \n",
       "1     the jollibee taste testing is the reason why i...  Positive   \n",
       "2           Yummy balut watching from Davao philippines  Positive   \n",
       "3     Mostly when a product is introduced or seen on...   Neutral   \n",
       "4     Hiiii can you do a challenge that you'll only ...   Neutral   \n",
       "...                                                 ...       ...   \n",
       "9995             Who here came back when they get bored   Neutral   \n",
       "9996  Kz sang iconic music in Philippines Emotional ...  Positive   \n",
       "9997                                   The piggy though  Negative   \n",
       "9998  In the Philippines you could buy a whole Lecho...   Neutral   \n",
       "9999                                              HARAM       NaN   \n",
       "\n",
       "                                    Category  \n",
       "0                                        NaN  \n",
       "1                 Praise towards the creator  \n",
       "2      Conversation/Anecdote/Opinion/Queries  \n",
       "3                                Information  \n",
       "4     Request/Suggestion towards the creator  \n",
       "...                                      ...  \n",
       "9995   Conversation/Anecdote/Opinion/Queries  \n",
       "9996   Conversation/Anecdote/Opinion/Queries  \n",
       "9997   Conversation/Anecdote/Opinion/Queries  \n",
       "9998                             Information  \n",
       "9999                                     NaN  \n",
       "\n",
       "[10000 rows x 4 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Train Dataset - Sheet2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do not drop N/A Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Praise towards the creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yummy balut watching from Davao, philippines</td>\n",
       "      <td>Yummy balut watching from Davao philippines</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mostly, when a product is introduced or seen o...</td>\n",
       "      <td>Mostly when a product is introduced or seen on...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi√¨iii can you do a challenge that you&amp;#39;ll ...</td>\n",
       "      <td>Hiiii can you do a challenge that you'll only ...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Request/Suggestion towards the creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will go to jollibee tomorrow because of this...</td>\n",
       "      <td>i will go to jollibee tomorrow because of this...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8782</th>\n",
       "      <td>I&amp;#39;m watching this late at night and it&amp;#39...</td>\n",
       "      <td>I'm watching this late at night and it's makin...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8783</th>\n",
       "      <td>Who here came back when they get bored?</td>\n",
       "      <td>Who here came back when they get bored</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8784</th>\n",
       "      <td>Kz sang iconic music in Philippines.. Emotiona...</td>\n",
       "      <td>Kz sang iconic music in Philippines Emotional ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8785</th>\n",
       "      <td>The piggy though ;(</td>\n",
       "      <td>The piggy though</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8786</th>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8787 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  \\\n",
       "0     the jollibee taste testing is the reason why i...   \n",
       "1          Yummy balut watching from Davao, philippines   \n",
       "2     Mostly, when a product is introduced or seen o...   \n",
       "3     Hi√¨iii can you do a challenge that you&#39;ll ...   \n",
       "4     i will go to jollibee tomorrow because of this...   \n",
       "...                                                 ...   \n",
       "8782  I&#39;m watching this late at night and it&#39...   \n",
       "8783            Who here came back when they get bored?   \n",
       "8784  Kz sang iconic music in Philippines.. Emotiona...   \n",
       "8785                                The piggy though ;(   \n",
       "8786  In the Philippines you could buy a whole Lecho...   \n",
       "\n",
       "                                          clean_comment  Polarity  \\\n",
       "0     the jollibee taste testing is the reason why i...  Positive   \n",
       "1           Yummy balut watching from Davao philippines  Positive   \n",
       "2     Mostly when a product is introduced or seen on...   Neutral   \n",
       "3     Hiiii can you do a challenge that you'll only ...   Neutral   \n",
       "4     i will go to jollibee tomorrow because of this...  Positive   \n",
       "...                                                 ...       ...   \n",
       "8782  I'm watching this late at night and it's makin...  Positive   \n",
       "8783             Who here came back when they get bored   Neutral   \n",
       "8784  Kz sang iconic music in Philippines Emotional ...  Positive   \n",
       "8785                                   The piggy though  Negative   \n",
       "8786  In the Philippines you could buy a whole Lecho...   Neutral   \n",
       "\n",
       "                                    Category  \n",
       "0                 Praise towards the creator  \n",
       "1      Conversation/Anecdote/Opinion/Queries  \n",
       "2                                Information  \n",
       "3     Request/Suggestion towards the creator  \n",
       "4      Conversation/Anecdote/Opinion/Queries  \n",
       "...                                      ...  \n",
       "8782   Conversation/Anecdote/Opinion/Queries  \n",
       "8783   Conversation/Anecdote/Opinion/Queries  \n",
       "8784   Conversation/Anecdote/Opinion/Queries  \n",
       "8785   Conversation/Anecdote/Opinion/Queries  \n",
       "8786                             Information  \n",
       "\n",
       "[8787 rows x 4 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.dropna(inplace=True)\n",
    "# df.reset_index(drop=True, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process dataset\n",
    "- remove special characters and emojis \n",
    "- lowercase all texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    text = \" \".join(filtered_tokens)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = text.split()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    text = \" \".join(lemmatized_tokens)\n",
    "    return text\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    # Drop rows where either \"comment\" or \"clean_comment\" is empty\n",
    "    df.dropna(subset=[\"comment\", \"clean_comment\"], inplace=True)\n",
    "    \n",
    "    # Handle NaN values for other columns\n",
    "    df[\"Polarity\"].fillna(\"Unknown\", inplace=True)\n",
    "    df[\"Category\"].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Apply preprocessing function\n",
    "    df[\"clean_comment\"] = df[\"clean_comment\"].apply(preprocess_text)\n",
    "    \n",
    "    # Reset index if needed\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>jollibee taste testing reason im lol love vlog...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Praise towards the creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yummy balut watching from Davao, philippines</td>\n",
       "      <td>yummy balut watching davao philippine</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mostly, when a product is introduced or seen o...</td>\n",
       "      <td>mostly product introduced seen tv first time l...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi√¨iii can you do a challenge that you&amp;#39;ll ...</td>\n",
       "      <td>hiiii challenge youll speak filipino fam sibli...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Request/Suggestion towards the creator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will go to jollibee tomorrow because of this...</td>\n",
       "      <td>go jollibee tomorrow thisi love spicy chicken ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>Who here came back when they get bored?</td>\n",
       "      <td>came back get bored</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>Kz sang iconic music in Philippines.. Emotiona...</td>\n",
       "      <td>kz sang iconic music philippine emotional amazing</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>The piggy though ;(</td>\n",
       "      <td>piggy though</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>philippine could buy whole lechon dollar weigh...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>HARAM</td>\n",
       "      <td>haram</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9718 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  \\\n",
       "0     the jollibee taste testing is the reason why i...   \n",
       "1          Yummy balut watching from Davao, philippines   \n",
       "2     Mostly, when a product is introduced or seen o...   \n",
       "3     Hi√¨iii can you do a challenge that you&#39;ll ...   \n",
       "4     i will go to jollibee tomorrow because of this...   \n",
       "...                                                 ...   \n",
       "9713            Who here came back when they get bored?   \n",
       "9714  Kz sang iconic music in Philippines.. Emotiona...   \n",
       "9715                                The piggy though ;(   \n",
       "9716  In the Philippines you could buy a whole Lecho...   \n",
       "9717                                              HARAM   \n",
       "\n",
       "                                          clean_comment  Polarity  \\\n",
       "0     jollibee taste testing reason im lol love vlog...  Positive   \n",
       "1                 yummy balut watching davao philippine  Positive   \n",
       "2     mostly product introduced seen tv first time l...   Neutral   \n",
       "3     hiiii challenge youll speak filipino fam sibli...   Neutral   \n",
       "4     go jollibee tomorrow thisi love spicy chicken ...  Positive   \n",
       "...                                                 ...       ...   \n",
       "9713                                came back get bored   Neutral   \n",
       "9714  kz sang iconic music philippine emotional amazing  Positive   \n",
       "9715                                       piggy though  Negative   \n",
       "9716  philippine could buy whole lechon dollar weigh...   Neutral   \n",
       "9717                                              haram   Unknown   \n",
       "\n",
       "                                    Category  \n",
       "0                 Praise towards the creator  \n",
       "1      Conversation/Anecdote/Opinion/Queries  \n",
       "2                                Information  \n",
       "3     Request/Suggestion towards the creator  \n",
       "4      Conversation/Anecdote/Opinion/Queries  \n",
       "...                                      ...  \n",
       "9713   Conversation/Anecdote/Opinion/Queries  \n",
       "9714   Conversation/Anecdote/Opinion/Queries  \n",
       "9715   Conversation/Anecdote/Opinion/Queries  \n",
       "9716                             Information  \n",
       "9717                                 Unknown  \n",
       "\n",
       "[9718 rows x 4 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_dataframe(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] =  df['Category'].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories \n",
    "- Label 0 : Conversation/Anecdote/Opinion/Queries\n",
    "- Label 1 : Criticism towards the creator\n",
    "- Label 2 : Information\n",
    "- Label 3 : Praise towards the creator\n",
    "- Label 4 : Request/Suggestion towards the creator\n",
    "- Label 5 : Unknown (N/A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Praise towards the creator',\n",
       "       'Conversation/Anecdote/Opinion/Queries', 'Information',\n",
       "       'Request/Suggestion towards the creator',\n",
       "       'Criticism towards the creator', 'Unknown'], dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 2, 4, 1, 5], dtype=int8)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Category</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>jollibee taste testing reason im lol love vlog...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Praise towards the creator</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yummy balut watching from Davao, philippines</td>\n",
       "      <td>yummy balut watching davao philippine</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mostly, when a product is introduced or seen o...</td>\n",
       "      <td>mostly product introduced seen tv first time l...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi√¨iii can you do a challenge that you&amp;#39;ll ...</td>\n",
       "      <td>hiiii challenge youll speak filipino fam sibli...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Request/Suggestion towards the creator</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will go to jollibee tomorrow because of this...</td>\n",
       "      <td>go jollibee tomorrow thisi love spicy chicken ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>Who here came back when they get bored?</td>\n",
       "      <td>came back get bored</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>Kz sang iconic music in Philippines.. Emotiona...</td>\n",
       "      <td>kz sang iconic music philippine emotional amazing</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>The piggy though ;(</td>\n",
       "      <td>piggy though</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>philippine could buy whole lechon dollar weigh...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>HARAM</td>\n",
       "      <td>haram</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9718 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  \\\n",
       "0     the jollibee taste testing is the reason why i...   \n",
       "1          Yummy balut watching from Davao, philippines   \n",
       "2     Mostly, when a product is introduced or seen o...   \n",
       "3     Hi√¨iii can you do a challenge that you&#39;ll ...   \n",
       "4     i will go to jollibee tomorrow because of this...   \n",
       "...                                                 ...   \n",
       "9713            Who here came back when they get bored?   \n",
       "9714  Kz sang iconic music in Philippines.. Emotiona...   \n",
       "9715                                The piggy though ;(   \n",
       "9716  In the Philippines you could buy a whole Lecho...   \n",
       "9717                                              HARAM   \n",
       "\n",
       "                                          clean_comment  Polarity  \\\n",
       "0     jollibee taste testing reason im lol love vlog...  Positive   \n",
       "1                 yummy balut watching davao philippine  Positive   \n",
       "2     mostly product introduced seen tv first time l...   Neutral   \n",
       "3     hiiii challenge youll speak filipino fam sibli...   Neutral   \n",
       "4     go jollibee tomorrow thisi love spicy chicken ...  Positive   \n",
       "...                                                 ...       ...   \n",
       "9713                                came back get bored   Neutral   \n",
       "9714  kz sang iconic music philippine emotional amazing  Positive   \n",
       "9715                                       piggy though  Negative   \n",
       "9716  philippine could buy whole lechon dollar weigh...   Neutral   \n",
       "9717                                              haram   Unknown   \n",
       "\n",
       "                                    Category  label  \n",
       "0                 Praise towards the creator      3  \n",
       "1      Conversation/Anecdote/Opinion/Queries      0  \n",
       "2                                Information      2  \n",
       "3     Request/Suggestion towards the creator      4  \n",
       "4      Conversation/Anecdote/Opinion/Queries      0  \n",
       "...                                      ...    ...  \n",
       "9713   Conversation/Anecdote/Opinion/Queries      0  \n",
       "9714   Conversation/Anecdote/Opinion/Queries      0  \n",
       "9715   Conversation/Anecdote/Opinion/Queries      0  \n",
       "9716                             Information      2  \n",
       "9717                                 Unknown      5  \n",
       "\n",
       "[9718 rows x 5 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = '__label__' + df['label'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>clean_comment</th>\n",
       "      <th>Polarity</th>\n",
       "      <th>Category</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the jollibee taste testing is the reason why i...</td>\n",
       "      <td>jollibee taste testing reason im lol love vlog...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Praise towards the creator</td>\n",
       "      <td>__label__3</td>\n",
       "      <td>__label__3 jollibee taste testing reason im lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yummy balut watching from Davao, philippines</td>\n",
       "      <td>yummy balut watching davao philippine</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>__label__0</td>\n",
       "      <td>__label__0 yummy balut watching davao philippine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mostly, when a product is introduced or seen o...</td>\n",
       "      <td>mostly product introduced seen tv first time l...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>__label__2 mostly product introduced seen tv f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hi√¨iii can you do a challenge that you&amp;#39;ll ...</td>\n",
       "      <td>hiiii challenge youll speak filipino fam sibli...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Request/Suggestion towards the creator</td>\n",
       "      <td>__label__4</td>\n",
       "      <td>__label__4 hiiii challenge youll speak filipin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i will go to jollibee tomorrow because of this...</td>\n",
       "      <td>go jollibee tomorrow thisi love spicy chicken ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>__label__0</td>\n",
       "      <td>__label__0 go jollibee tomorrow thisi love spi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9713</th>\n",
       "      <td>Who here came back when they get bored?</td>\n",
       "      <td>came back get bored</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>__label__0</td>\n",
       "      <td>__label__0 came back get bored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9714</th>\n",
       "      <td>Kz sang iconic music in Philippines.. Emotiona...</td>\n",
       "      <td>kz sang iconic music philippine emotional amazing</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>__label__0</td>\n",
       "      <td>__label__0 kz sang iconic music philippine emo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9715</th>\n",
       "      <td>The piggy though ;(</td>\n",
       "      <td>piggy though</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Conversation/Anecdote/Opinion/Queries</td>\n",
       "      <td>__label__0</td>\n",
       "      <td>__label__0 piggy though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9716</th>\n",
       "      <td>In the Philippines you could buy a whole Lecho...</td>\n",
       "      <td>philippine could buy whole lechon dollar weigh...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Information</td>\n",
       "      <td>__label__2</td>\n",
       "      <td>__label__2 philippine could buy whole lechon d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9717</th>\n",
       "      <td>HARAM</td>\n",
       "      <td>haram</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>__label__5</td>\n",
       "      <td>__label__5 haram</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9718 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  \\\n",
       "0     the jollibee taste testing is the reason why i...   \n",
       "1          Yummy balut watching from Davao, philippines   \n",
       "2     Mostly, when a product is introduced or seen o...   \n",
       "3     Hi√¨iii can you do a challenge that you&#39;ll ...   \n",
       "4     i will go to jollibee tomorrow because of this...   \n",
       "...                                                 ...   \n",
       "9713            Who here came back when they get bored?   \n",
       "9714  Kz sang iconic music in Philippines.. Emotiona...   \n",
       "9715                                The piggy though ;(   \n",
       "9716  In the Philippines you could buy a whole Lecho...   \n",
       "9717                                              HARAM   \n",
       "\n",
       "                                          clean_comment  Polarity  \\\n",
       "0     jollibee taste testing reason im lol love vlog...  Positive   \n",
       "1                 yummy balut watching davao philippine  Positive   \n",
       "2     mostly product introduced seen tv first time l...   Neutral   \n",
       "3     hiiii challenge youll speak filipino fam sibli...   Neutral   \n",
       "4     go jollibee tomorrow thisi love spicy chicken ...  Positive   \n",
       "...                                                 ...       ...   \n",
       "9713                                came back get bored   Neutral   \n",
       "9714  kz sang iconic music philippine emotional amazing  Positive   \n",
       "9715                                       piggy though  Negative   \n",
       "9716  philippine could buy whole lechon dollar weigh...   Neutral   \n",
       "9717                                              haram   Unknown   \n",
       "\n",
       "                                    Category       label  \\\n",
       "0                 Praise towards the creator  __label__3   \n",
       "1      Conversation/Anecdote/Opinion/Queries  __label__0   \n",
       "2                                Information  __label__2   \n",
       "3     Request/Suggestion towards the creator  __label__4   \n",
       "4      Conversation/Anecdote/Opinion/Queries  __label__0   \n",
       "...                                      ...         ...   \n",
       "9713   Conversation/Anecdote/Opinion/Queries  __label__0   \n",
       "9714   Conversation/Anecdote/Opinion/Queries  __label__0   \n",
       "9715   Conversation/Anecdote/Opinion/Queries  __label__0   \n",
       "9716                             Information  __label__2   \n",
       "9717                                 Unknown  __label__5   \n",
       "\n",
       "                                             label_text  \n",
       "0     __label__3 jollibee taste testing reason im lo...  \n",
       "1      __label__0 yummy balut watching davao philippine  \n",
       "2     __label__2 mostly product introduced seen tv f...  \n",
       "3     __label__4 hiiii challenge youll speak filipin...  \n",
       "4     __label__0 go jollibee tomorrow thisi love spi...  \n",
       "...                                                 ...  \n",
       "9713                     __label__0 came back get bored  \n",
       "9714  __label__0 kz sang iconic music philippine emo...  \n",
       "9715                            __label__0 piggy though  \n",
       "9716  __label__2 philippine could buy whole lechon d...  \n",
       "9717                                   __label__5 haram  \n",
       "\n",
       "[9718 rows x 6 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label_text'] = df['label'] + ' ' + df['clean_comment']\n",
    "df['label_text'] = df['label_text'].str.rstrip()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare train, test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "train, valid = train_test_split(train, test_size=0.2, random_state=42, stratify = train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"content/train.csv\", columns=[\"label\",\"clean_comment\"], index=False, header=False)\n",
    "valid.to_csv(\"content/dev.csv\", columns=[\"label\",\"clean_comment\"], index=False, header=False)\n",
    "test.to_csv(\"content/test.csv\", columns=[\"label\",\"clean_comment\"], index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dictionary / corpus for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [(str(row['clean_comment']), str(row['label'])) for index, row in train.iterrows()]\n",
    "data_test = [(str(row['clean_comment']), str(row['label'])) for index, row in test.iterrows()]\n",
    "data_valid = [(str(row['clean_comment']), str(row['label'])) for index, row in valid.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name_map = {0: 'text', 1: 'label'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_corpus_dir = 'flair_corpus'\n",
    "os.makedirs(flair_corpus_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from pathlib import Path\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "from flair.samplers import ImbalancedClassificationDatasetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_csv_file = os.path.join(flair_corpus_dir, 'train.csv')\n",
    "with open(flair_csv_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('text,label\\n')\n",
    "    for row in data_train:\n",
    "        f.write(f'{row[0]},{row[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_csv_file = os.path.join(flair_corpus_dir, 'test.csv')\n",
    "with open(flair_csv_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('text,label\\n')\n",
    "    for row in data_test:\n",
    "        f.write(f'{row[0]},{row[1]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "flair_csv_file = os.path.join(flair_corpus_dir, 'valid.csv')\n",
    "with open(flair_csv_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('text,label\\n')\n",
    "    for row in data_valid:\n",
    "        f.write(f'{row[0]},{row[1]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:03,144 Reading data from flair_corpus\n",
      "2024-02-29 00:01:03,145 Train: flair_corpus\\train.csv\n",
      "2024-02-29 00:01:03,145 Dev: None\n",
      "2024-02-29 00:01:03,146 Test: flair_corpus\\test.csv\n",
      "2024-02-29 00:01:03,177 No dev split found. Using 0% (i.e. 622 samples) of the train split as dev data\n",
      "2024-02-29 00:01:03,178 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:03,206 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-29 00:01:03,255 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-29 00:01:03,267 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "531it [00:00, 5305.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:03,303 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1062it [00:00, 4274.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:03,461 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4575it [00:00, 5825.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:04,075 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5209it [00:00, 5982.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:04,186 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5598it [00:01, 5570.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-29 00:01:04,191 Dictionary created for label 'label' with 7 values: __label__0 (seen 2515 times), __label__3 (seen 983 times), __label__4 (seen 864 times), __label__5 (seen 543 times), __label__2 (seen 537 times), __label__1 (seen 155 times), label (seen 1 times)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label_type = 'label'\n",
    "# load corpus containing training, test and dev data\n",
    "corpus = CSVClassificationCorpus(flair_corpus_dir, column_name_map, label_type=label_type)\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=True)\n",
    "\n",
    "# Create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type=label_type)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 16:57:58,186 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 16:57:58,673 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 16:57:58,872 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 16:57:59,391 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 16:57:59,521 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,523 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30523, 768)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-02-28 16:57:59,523 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,524 Corpus: 4017 train + 446 dev + 1396 test sentences\n",
      "2024-02-28 16:57:59,524 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,525 Train:  4463 sentences\n",
      "2024-02-28 16:57:59,526         (train_with_dev=True, train_with_test=False)\n",
      "2024-02-28 16:57:59,526 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,527 Training Params:\n",
      "2024-02-28 16:57:59,527  - learning_rate: \"0.001\" \n",
      "2024-02-28 16:57:59,527  - mini_batch_size: \"16\"\n",
      "2024-02-28 16:57:59,528  - max_epochs: \"1\"\n",
      "2024-02-28 16:57:59,528  - shuffle: \"False\"\n",
      "2024-02-28 16:57:59,529 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,529 Plugins:\n",
      "2024-02-28 16:57:59,530  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
      "2024-02-28 16:57:59,530 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,530 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-02-28 16:57:59,531  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-02-28 16:57:59,531 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,532 Computation:\n",
      "2024-02-28 16:57:59,533  - compute on device: cpu\n",
      "2024-02-28 16:57:59,533  - embedding storage: gpu\n",
      "2024-02-28 16:57:59,533 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,534 Model training base path: \"content\\flair\"\n",
      "2024-02-28 16:57:59,534 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:57:59,535 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 16:59:59,636 epoch 1 - iter 27/279 - loss 0.79036086 - time (sec): 120.10 - samples/sec: 3.60 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:02:05,838 epoch 1 - iter 54/279 - loss 0.78621450 - time (sec): 246.30 - samples/sec: 3.51 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:04:11,390 epoch 1 - iter 81/279 - loss 0.78534382 - time (sec): 371.85 - samples/sec: 3.49 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:06:24,573 epoch 1 - iter 108/279 - loss 0.75815894 - time (sec): 505.04 - samples/sec: 3.42 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:08:08,165 epoch 1 - iter 135/279 - loss 0.74561882 - time (sec): 608.63 - samples/sec: 3.55 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:10:24,903 epoch 1 - iter 162/279 - loss 0.75603124 - time (sec): 745.37 - samples/sec: 3.48 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:12:30,403 epoch 1 - iter 189/279 - loss 0.75510366 - time (sec): 870.87 - samples/sec: 3.47 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:14:35,241 epoch 1 - iter 216/279 - loss 0.75840053 - time (sec): 995.71 - samples/sec: 3.47 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:16:41,127 epoch 1 - iter 243/279 - loss 0.75449770 - time (sec): 1121.59 - samples/sec: 3.47 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:18:41,266 epoch 1 - iter 270/279 - loss 0.74890550 - time (sec): 1241.73 - samples/sec: 3.48 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:19:22,744 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:19:22,745 EPOCH 1 done: loss 0.7486 - lr: 0.001000\n",
      "2024-02-28 17:19:22,747  - 0 epochs without improvement\n",
      "2024-02-28 17:19:22,749  - 0 epochs without improvement\n",
      "2024-02-28 17:19:22,751  - 0 epochs without improvement\n",
      "2024-02-28 17:19:22,753  - 0 epochs without improvement\n",
      "2024-02-28 17:19:23,316 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:19:23,318 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [11:03<00:00, 30.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 17:30:26,444 \n",
      "Results:\n",
      "- F-score (micro) 0.5838\n",
      "- F-score (macro) 0.6149\n",
      "- Accuracy 0.5838\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  __label__0     0.8476    0.4847    0.6167       815\n",
      "  __label__2     0.4709    0.6667    0.5519       291\n",
      "  __label__3     0.6273    0.8313    0.7150       243\n",
      "  __label__1     0.1179    0.5000    0.1909        46\n",
      "       label     1.0000    1.0000    1.0000         1\n",
      "\n",
      "    accuracy                         0.5838      1396\n",
      "   macro avg     0.6128    0.6965    0.6149      1396\n",
      "weighted avg     0.7068    0.5838    0.6066      1396\n",
      "\n",
      "2024-02-28 17:30:26,446 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 17:31:05,337 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Epoch 1: Validation Accuracy - 58.92%\n",
      "2024-02-28 17:31:57,648 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 17:31:58,152 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 17:31:58,360 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 17:31:58,851 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 17:31:58,949 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,951 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30523, 768)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-02-28 17:31:58,951 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,952 Corpus: 4017 train + 446 dev + 1396 test sentences\n",
      "2024-02-28 17:31:58,952 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,953 Train:  4463 sentences\n",
      "2024-02-28 17:31:58,953         (train_with_dev=True, train_with_test=False)\n",
      "2024-02-28 17:31:58,953 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,954 Training Params:\n",
      "2024-02-28 17:31:58,954  - learning_rate: \"0.001\" \n",
      "2024-02-28 17:31:58,955  - mini_batch_size: \"16\"\n",
      "2024-02-28 17:31:58,955  - max_epochs: \"1\"\n",
      "2024-02-28 17:31:58,956  - shuffle: \"False\"\n",
      "2024-02-28 17:31:58,956 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,957 Plugins:\n",
      "2024-02-28 17:31:58,958  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
      "2024-02-28 17:31:58,958 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,959 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-02-28 17:31:58,959  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-02-28 17:31:58,959 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,960 Computation:\n",
      "2024-02-28 17:31:58,960  - compute on device: cpu\n",
      "2024-02-28 17:31:58,961  - embedding storage: gpu\n",
      "2024-02-28 17:31:58,961 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,962 Model training base path: \"content\\flair\"\n",
      "2024-02-28 17:31:58,962 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:31:58,963 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:33:21,599 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 17:34:00,163 epoch 1 - iter 27/279 - loss 0.66652327 - time (sec): 121.20 - samples/sec: 3.56 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:35:56,815 epoch 1 - iter 54/279 - loss 0.65952837 - time (sec): 237.85 - samples/sec: 3.63 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:37:35,729 epoch 1 - iter 81/279 - loss 0.66442040 - time (sec): 336.76 - samples/sec: 3.85 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:39:29,816 epoch 1 - iter 108/279 - loss 0.67359219 - time (sec): 450.85 - samples/sec: 3.83 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:41:19,724 epoch 1 - iter 135/279 - loss 0.66950590 - time (sec): 560.76 - samples/sec: 3.85 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:43:13,785 epoch 1 - iter 162/279 - loss 0.66952718 - time (sec): 674.82 - samples/sec: 3.84 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:45:13,248 epoch 1 - iter 189/279 - loss 0.66769548 - time (sec): 794.28 - samples/sec: 3.81 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:47:05,816 epoch 1 - iter 216/279 - loss 0.67388342 - time (sec): 906.85 - samples/sec: 3.81 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:48:51,020 epoch 1 - iter 243/279 - loss 0.68221424 - time (sec): 1012.06 - samples/sec: 3.84 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:50:34,833 epoch 1 - iter 270/279 - loss 0.68129559 - time (sec): 1115.87 - samples/sec: 3.87 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 17:51:09,876 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:51:09,876 EPOCH 1 done: loss 0.6766 - lr: 0.001000\n",
      "2024-02-28 17:51:09,878  - 0 epochs without improvement\n",
      "2024-02-28 17:51:09,880  - 0 epochs without improvement\n",
      "2024-02-28 17:51:09,881  - 0 epochs without improvement\n",
      "2024-02-28 17:51:09,882  - 0 epochs without improvement\n",
      "2024-02-28 17:51:09,884  - 0 epochs without improvement\n",
      "2024-02-28 17:51:10,341 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 17:51:10,342 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [09:19<00:00, 25.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 18:00:30,253 \n",
      "Results:\n",
      "- F-score (micro) 0.5201\n",
      "- F-score (macro) 0.5927\n",
      "- Accuracy 0.5201\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  __label__0     0.8793    0.3485    0.4991       815\n",
      "  __label__2     0.4866    0.6873    0.5698       291\n",
      "  __label__3     0.6220    0.8601    0.7219       243\n",
      "  __label__1     0.0985    0.6957    0.1725        46\n",
      "       label     1.0000    1.0000    1.0000         1\n",
      "\n",
      "    accuracy                         0.5201      1396\n",
      "   macro avg     0.6173    0.7183    0.5927      1396\n",
      "weighted avg     0.7270    0.5201    0.5422      1396\n",
      "\n",
      "2024-02-28 18:00:30,254 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 18:00:58,510 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Epoch 2: Validation Accuracy - 52.20%\n",
      "2024-02-28 18:01:40,529 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 18:01:40,983 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 18:01:41,174 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 18:01:41,610 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "2024-02-28 18:01:41,698 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,700 Model: \"TextClassifier(\n",
      "  (embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30523, 768)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (locked_dropout): LockedDropout(p=0.0)\n",
      "  (word_dropout): WordDropout(p=0.0)\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2024-02-28 18:01:41,700 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,701 Corpus: 4017 train + 446 dev + 1396 test sentences\n",
      "2024-02-28 18:01:41,701 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,702 Train:  4463 sentences\n",
      "2024-02-28 18:01:41,702         (train_with_dev=True, train_with_test=False)\n",
      "2024-02-28 18:01:41,703 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,703 Training Params:\n",
      "2024-02-28 18:01:41,703  - learning_rate: \"0.001\" \n",
      "2024-02-28 18:01:41,704  - mini_batch_size: \"16\"\n",
      "2024-02-28 18:01:41,704  - max_epochs: \"1\"\n",
      "2024-02-28 18:01:41,705  - shuffle: \"False\"\n",
      "2024-02-28 18:01:41,705 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,705 Plugins:\n",
      "2024-02-28 18:01:41,706  - AnnealOnPlateau | patience: '3', anneal_factor: '0.5', min_learning_rate: '0.0001'\n",
      "2024-02-28 18:01:41,706 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,706 Final evaluation on model from best epoch (best-model.pt)\n",
      "2024-02-28 18:01:41,707  - metric: \"('micro avg', 'f1-score')\"\n",
      "2024-02-28 18:01:41,707 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,708 Computation:\n",
      "2024-02-28 18:01:41,708  - compute on device: cpu\n",
      "2024-02-28 18:01:41,708  - embedding storage: gpu\n",
      "2024-02-28 18:01:41,709 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,709 Model training base path: \"content\\flair\"\n",
      "2024-02-28 18:01:41,709 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:01:41,710 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:03:40,370 epoch 1 - iter 27/279 - loss 0.61187962 - time (sec): 118.66 - samples/sec: 3.64 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:05:38,878 epoch 1 - iter 54/279 - loss 0.63758403 - time (sec): 237.17 - samples/sec: 3.64 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:07:16,311 epoch 1 - iter 81/279 - loss 0.63155417 - time (sec): 334.60 - samples/sec: 3.87 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:09:09,341 epoch 1 - iter 108/279 - loss 0.63754298 - time (sec): 447.63 - samples/sec: 3.86 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:10:52,357 epoch 1 - iter 135/279 - loss 0.63359240 - time (sec): 550.65 - samples/sec: 3.92 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:12:59,205 epoch 1 - iter 162/279 - loss 0.62047022 - time (sec): 677.49 - samples/sec: 3.83 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:14:47,389 epoch 1 - iter 189/279 - loss 0.61554009 - time (sec): 785.68 - samples/sec: 3.85 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:16:33,534 epoch 1 - iter 216/279 - loss 0.60563836 - time (sec): 891.82 - samples/sec: 3.88 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:18:20,038 epoch 1 - iter 243/279 - loss 0.60576184 - time (sec): 998.33 - samples/sec: 3.89 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:20:11,988 epoch 1 - iter 270/279 - loss 0.59949530 - time (sec): 1110.28 - samples/sec: 3.89 - lr: 0.001000 - momentum: 0.000000\n",
      "2024-02-28 18:20:45,831 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:20:45,832 EPOCH 1 done: loss 0.6015 - lr: 0.001000\n",
      "2024-02-28 18:20:45,833  - 0 epochs without improvement\n",
      "2024-02-28 18:20:45,835  - 0 epochs without improvement\n",
      "2024-02-28 18:20:45,836  - 0 epochs without improvement\n",
      "2024-02-28 18:20:45,838  - 0 epochs without improvement\n",
      "2024-02-28 18:20:45,839  - 0 epochs without improvement\n",
      "2024-02-28 18:20:45,840  - 0 epochs without improvement\n",
      "2024-02-28 18:20:46,288 ----------------------------------------------------------------------------------------------------\n",
      "2024-02-28 18:20:46,290 Testing using last state of model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [09:18<00:00, 25.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 18:30:04,746 \n",
      "Results:\n",
      "- F-score (micro) 0.6074\n",
      "- F-score (macro) 0.6314\n",
      "- Accuracy 0.6074\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  __label__0     0.8640    0.5067    0.6388       815\n",
      "  __label__2     0.5104    0.6770    0.5820       291\n",
      "  __label__3     0.6127    0.8724    0.7199       243\n",
      "  __label__1     0.1351    0.5435    0.2165        46\n",
      "       label     1.0000    1.0000    1.0000         1\n",
      "\n",
      "    accuracy                         0.6074      1396\n",
      "   macro avg     0.6244    0.7199    0.6314      1396\n",
      "weighted avg     0.7226    0.6074    0.6274      1396\n",
      "\n",
      "2024-02-28 18:30:04,747 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-28 18:30:33,061 Warning: An empty Sentence was created! Are there empty strings in your dataset?\n",
      "Epoch 3: Validation Accuracy - 59.91%\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "\n",
    "# Function to calculate accuracy on validation set\n",
    "def calculate_validation_accuracy(model, validation_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for text, label in validation_data:\n",
    "        # Convert text to Flair Sentence object\n",
    "        sentence = Sentence(text)\n",
    "        # Predict label for the sentence\n",
    "        model.predict(sentence)\n",
    "        \n",
    "        # Check if sentence has labels\n",
    "        if sentence.labels:\n",
    "            predicted_label = sentence.labels[0].value\n",
    "            if predicted_label == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    # Return accuracy only if predictions were made\n",
    "    if total > 0:\n",
    "        return correct / total\n",
    "    else:\n",
    "        return 0  # Return 0 accuracy if no predictions were made\n",
    "    \n",
    "def calculate_training_accuracy(model, training_data):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for text, label in training_data:\n",
    "        sentence = Sentence(text)\n",
    "        model.predict(sentence)\n",
    "        if sentence.labels:\n",
    "            predicted_label = sentence.labels[0].value\n",
    "            if predicted_label == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    if total > 0:\n",
    "        return correct / total\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "validation_accuracy_list = []\n",
    "max_epochs = 3\n",
    "\n",
    "for epoch in range(1, max_epochs + 1):\n",
    "    trainer.train('content/flair/', \n",
    "                  embeddings_storage_mode='gpu',\n",
    "                  learning_rate=0.001,  \n",
    "                  mini_batch_size=16,   \n",
    "                  mini_batch_chunk_size=4,\n",
    "                  sampler=ImbalancedClassificationDatasetSampler,\n",
    "                  train_with_dev=\"True\", \n",
    "                  max_epochs=1,  # Train for one epoch at a time\n",
    "                  )\n",
    "    \n",
    "    # Calculate validation and training accuracy\n",
    "    validation_accuracy = calculate_validation_accuracy(trainer.model, data_valid)\n",
    "    training_accuracy = calculate_training_accuracy(trainer.model, data_train)\n",
    "\n",
    "    \n",
    "    validation_accuracy_list.append(validation_accuracy)\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Training Accuracy - {training_accuracy * 100:.2f}% | Validation Accuracy - {validation_accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  __label__0       0.87      0.70      0.77       815\n",
      "  __label__1       0.39      0.26      0.31        46\n",
      "  __label__2       0.56      0.79      0.66       291\n",
      "  __label__3       0.69      0.85      0.76       243\n",
      "\n",
      "    accuracy                           0.73      1395\n",
      "   macro avg       0.63      0.65      0.63      1395\n",
      "weighted avg       0.76      0.73      0.73      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import TextClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model = TextClassifier.load('content/flair/final-model.pt')\n",
    "\n",
    "# Load the test dataset\n",
    "test_sentences = [text for text, label in data_test]\n",
    "actual_labels = [label for text, label in data_test]\n",
    "\n",
    "predicted_labels = []\n",
    "for text in test_sentences:\n",
    "    sentence = Sentence(text)\n",
    "    model.predict(sentence)\n",
    "    predicted_label = sentence.labels[0].value\n",
    "    predicted_labels.append(predicted_label)\n",
    "\n",
    "report = classification_report(actual_labels, predicted_labels)\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
